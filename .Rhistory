withMathJax(),  # Enables MathJax for rendering LaTeX math
h3("Understanding Grad-CAM (Gradient-weighted Class Activation Mapping)"),
p("Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique used to visualize and interpret the decisions made by convolutional neural networks (CNNs),
particularly in image classification tasks. While CNNs are highly effective in extracting features from images and making predictions, they are often viewed as 'black boxes'
due to the difficulty in understanding what parts of an image contributed to a specific prediction. Grad-CAM addresses this challenge by generating a visual heatmap
that highlights the regions in an image that most influenced the network's prediction."),
h4("Key Steps of Grad-CAM:"),
tags$ol(
tags$li("Input Image and Model Prediction: The process starts by feeding an input image through a CNN model (e.g., VGG16 or ResNet),
which extracts features and outputs a prediction across different classes. For a chosen class, Grad-CAM seeks to identify
which parts of the image contributed most to the score of that class."),
tags$li("Choosing the Last Convolutional Layer: Grad-CAM focuses on the feature maps in the last convolutional layer of the CNN.
This layer retains spatial information that is essential for visualizing 'where' certain features were detected in the image."),
tags$li("Computing Gradients: Grad-CAM calculates the gradient of the class score with respect to each feature map in the last convolutional layer.
Mathematically, this is represented as the gradient, $$ \\frac{\\partial y_c}{\\partial A_k} $$, where $$ y_c $$ is the score of the class of interest
and $$ A_k $$ is the activation of the $$ k $$-th feature map in the selected layer. This gradient highlights the sensitivity of the prediction to changes in each feature map."),
tags$li("Global Average Pooling of Gradients: The gradients for each feature map are averaged across all spatial locations to get a single importance weight for each map.
This weight is computed as: $$ \\alpha_k^c = \\frac{1}{Z} \\sum_{i,j} \\frac{\\partial y_c}{\\partial A_{k}^{ij}} $$,
where $$ Z $$ is the total number of spatial locations, and $$ \\alpha_k^c $$ represents the importance of each feature map $$ A_k $$ for the class $$ c $$."),
tags$li("Generating the Heatmap: Grad-CAM produces a weighted combination of feature maps by multiplying each feature map $$ A_k $$ by its corresponding weight $$ \\alpha_k^c $$.
The heatmap $$ L^c $$ is then calculated as: $$ L^c = \\text{ReLU} \\left( \\sum_k \\alpha_k^c A_k \\right) $$,
where $$ \\text{ReLU} $$ sets negative values to zero, focusing on positive contributions to the prediction. This step effectively highlights areas in the image that most support the model’s classification decision."),
tags$li("Overlaying the Heatmap: The heatmap is resized to match the original image and is superimposed to highlight areas that most influenced the model's decision.
Brighter regions indicate the model's focus, helping users interpret the relevance of different parts of the image.")
),
h4("Why Grad-CAM is Important:"),
tags$ul(
tags$li("Model Interpretability: Grad-CAM provides insights into the decision-making process of neural networks, making the model’s reasoning more transparent."),
tags$li("Debugging Models: By showing where the model focuses, Grad-CAM can reveal if the model is relying on irrelevant or misleading parts of the image,
helping identify issues in the model's understanding."),
tags$li("Trust and Transparency: This technique builds trust in model predictions by explaining how the model arrived at its decision, which is especially useful in critical applications.")
),
h4("Further Reading"),
p("For a comprehensive explanation of Grad-CAM, you can refer to the original research paper:"),
tags$a(href = "https://arxiv.org/abs/1610.02391", "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization")
)
)
)
)
)
)
# Chunk 4
# Define server logic
# server <- function(input, output, session) {
#
#   observeEvent(input$submit, {
#     req(input$file)  # Ensure an image file is uploaded
#
#     # Save the uploaded file to www folder with a fixed name for consistency
#     original_image_path <- file.path("www", "original_resized.png")
#     file.copy(input$file$datapath, original_image_path, overwrite = TRUE)
#
#     # Show progress bar while the model is running
#     withProgress(message = 'Analyzing image...', value = 0, {
#
#       # Step 1: Load the selected model based on user input
#       incProgress(0.2, detail = "Loading model")
#
#       # Call load_model from Python with the selected model name
#       python_model <- py$load_model(input$model_choice)
#       model <- python_model[[1]]
#       last_conv_layer_name <- python_model[[2]]
#       preprocess <- python_model[[3]]
#
#
#       # Step 2: Run Grad-CAM analysis with the specified model and parameters
#       incProgress(0.3, detail = "Running Grad-CAM")
#       source_python("C:/Users/rodri/OneDrive - St. Lawrence University/STAT289/SYE/model_script.py")  # Path to your Python script
#
#       # Call the Python function with the selected confidence threshold
#       results <- generate_gradcam(input$file$datapath, model, top_n=5, confidence_threshold=input$confidence, last_conv_layer_name=last_conv_layer_name)
#
#       print(paste("Results length:", length(results)))  # Debugging check
#
#       # Step 3: Display notifications and render results
#       if (length(results) == 0) {
#         showNotification("No predictions found above the confidence threshold.", type = "error")
#       } else {
#         incProgress(0.6, detail = "Rendering outputs")
#
#         # Display the uploaded image
#         output$uploaded_image <- renderImage({
#           list(src = original_image_path, alt = "Uploaded Image", width = "400px", height = "400px")
#         }, deleteFile = FALSE)
#
#         # Dynamically render predictions and their Grad-CAM images in the Prediction Results tab
#         output$predicted_results <- renderUI({
#           lapply(1:length(results), function(i) {
#             result <- results[[i]]
#
#             # Prepare Grad-CAM image rendering using renderImage
#             output[[paste0("gradcam_output_", i)]] <- renderImage({
#               if (file.exists(result$gradcam_img_path)) {
#                 list(src = result$gradcam_img_path, height = "300px", width = "300px", alt = "Grad-CAM Output")
#               } else {
#                 NULL  # Return NULL if the file doesn't exist
#               }
#             }, deleteFile = FALSE)
#
#             # Dynamically create UI elements for each prediction
#             tagList(
#               fluidRow(
#                 column(6,
#                        h4(paste("Predicted Category:", result$label)),
#                        h5(paste("Confidence Score:", round(result$confidence * 100, 2), "%"))
#                 ),
#                 column(6,
#                        h4("Grad-CAM Output:"),
#                        imageOutput(paste0("gradcam_output_", i), height = "300px", width = "300px")
#                 )
#               ),
#               hr()  # Add a horizontal line between predictions
#             )
#           })
#         })
#
#         incProgress(1, detail = "Complete!")
#       }
#     })
#   })
# }
# Chunk 5
server <- function(input, output, session) {
observeEvent(input$submit, {
req(input$file)
# Save the uploaded file to www folder with a fixed name for consistency
original_image_path <- file.path("www", "original_resized.png")
file.copy(input$file$datapath, original_image_path, overwrite = TRUE)
# Load the Python script with load_model and generate_gradcam functions
py_run_file("C:/Users/rodri/OneDrive - St. Lawrence University/STAT289/SYE/model_script.py")
if (is.null(py$load_model) || is.null(py$generate_gradcam)) {
showNotification("Error loading Python functions. Check script path or function definitions.", type = "error")
return()
}
# Show progress bar while the model is running
withProgress(message = 'Analyzing image...', value = 0, {
incProgress(0.2, detail = "Loading model")
# Load the model, layer, preprocessing function, and input size
python_model <- py$load_model(input$model_choice)
model <- python_model[[1]]
last_conv_layer_name <- python_model[[2]]
preprocess <- python_model[[3]]
input_size <- python_model[[4]]
incProgress(0.3, detail = "Running Grad-CAM")
# Call generate_gradcam with model and preprocess arguments
results <- py$generate_gradcam(input$file$datapath, model, preprocess, top_n=5, confidence_threshold=input$confidence, last_conv_layer_name=last_conv_layer_name, input_size=input_size)
if (length(results) == 0) {
showNotification("No predictions found above the confidence threshold.", type = "error")
} else {
incProgress(0.6, detail = "Rendering outputs")
# Update the original image every time a new file is uploaded
output$uploaded_image <- renderImage({
list(src = original_image_path, alt = "Uploaded Image", width = "400px", height = "400px")
}, deleteFile = FALSE)
# Dynamically render predictions and their Grad-CAM images in the Prediction Results tab
output$predicted_results <- renderUI({
lapply(1:length(results), function(i) {
result <- results[[i]]
# Prepare Grad-CAM image rendering using renderImage
output[[paste0("gradcam_output_", i)]] <- renderImage({
# Ensure the image file exists before serving it
if (file.exists(result$gradcam_img_path)) {
list(src = result$gradcam_img_path, height = "300px", width = "300px", alt = "Grad-CAM Output")
} else {
NULL  # Return NULL if the file doesn't exist
}
}, deleteFile = FALSE)
# Dynamically create UI elements for each prediction
tagList(
fluidRow(
column(6,
h4(paste("Predicted Category:", result$label)),
h5(paste("Confidence Score:", round(result$confidence * 100, 2), "%"))
),
column(6,
h4("Grad-CAM Output:"),
# Output the image using imageOutput instead of img()
imageOutput(paste0("gradcam_output_", i), height = "300px", width = "300px")
)
),
hr()  # Add a horizontal line between predictions
)
})
})
incProgress(1, detail = "Complete!")
}
})
})
}
# Chunk 6
# Run the Shiny app
shinyApp(ui = ui, server = server)
# Chunk 1
library(shiny)
library(shinydashboard)
library(reticulate)
library(png)
library(shinythemes)
library(shinycssloaders)
# Chunk 2
# Set Python path
use_python("C:/Users/rodri/AppData/Local/Programs/Python/Python39/python.exe")
# Chunk 3
# Define UI
ui <- fluidPage(
titlePanel("Object Recognition App"),
sidebarLayout(
sidebarPanel(
# Upload input
fileInput("file", "Upload an Image", accept = c("image/png", "image/jpeg")),
# Add a model selection input to the UI
selectInput("model_choice", "Choose a Model:",
choices = list("VGG16" = "vgg16",
"ResNet50" = "resnet50",
"MobileNetV2" = "mobilenet_v2",
"EfficientNetB0" = "efficientnetb0",
"EfficientNetB7" = "efficientnetb7")),
# Confidence threshold slider
sliderInput("confidence", "Confidence Threshold", min = 0, max = 1, value = 0.5),
# Action button for analysis
actionButton("submit", "Analyze Image", class = "btn-primary")
),
mainPanel(
# Use tabsetPanel to create two tabs: one for Original Image, one for Prediction Results
tabsetPanel(
# First tab for the original image
tabPanel("Original Image",
wellPanel(
h4("Original Image:"),
# Add spinner for loading status
withSpinner(imageOutput("uploaded_image", height = "400px", width = "400px"))
)
),
# Second tab for prediction results
tabPanel("Prediction Results",
wellPanel(
h3("Prediction Results"),
# Multiple predictions and Grad-CAM outputs
uiOutput("predicted_results")
)
),
# Add new "How to Use" tab
tabPanel(
"How to Use",
wellPanel(
h3("Instructions: How to Use the App"),
p("This application allows users to upload an image and get predictions of the object's category using a pre-trained neural network."),
h4("Steps to Use the Application:"),
tags$ol(
tags$li("Upload an image by clicking on the 'Browse' button in the 'Original Image' tab."),
tags$li("Adjust the confidence threshold using the slider. This allows you to filter out predictions that are below a certain confidence score."),
tags$li("Click on the 'Analyze Image' button to start the prediction process."),
tags$li("The results will be displayed in the 'Prediction Results' tab, along with Grad-CAM visualizations that highlight which parts of the image contributed to the prediction.")
),
h4("Notes:"),
tags$ul(
tags$li("The model used for this application is VGG16, pre-trained on the ImageNet dataset."),
tags$li("The confidence threshold slider helps in refining the displayed predictions based on confidence scores."),
tags$li("Ensure your image is in .png or .jpeg format before uploading.")
)
)
),
# How Grad-CAM Works Tab
tabPanel(
"How Grad-CAM Works",
mainPanel(
withMathJax(),  # Enables MathJax for rendering LaTeX math
h3("Understanding Grad-CAM (Gradient-weighted Class Activation Mapping)"),
p("Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique used to visualize and interpret the decisions made by convolutional neural networks (CNNs),
particularly in image classification tasks. While CNNs are highly effective in extracting features from images and making predictions, they are often viewed as 'black boxes'
due to the difficulty in understanding what parts of an image contributed to a specific prediction. Grad-CAM addresses this challenge by generating a visual heatmap
that highlights the regions in an image that most influenced the network's prediction."),
h4("Key Steps of Grad-CAM:"),
tags$ol(
tags$li("Input Image and Model Prediction: The process starts by feeding an input image through a CNN model (e.g., VGG16 or ResNet),
which extracts features and outputs a prediction across different classes. For a chosen class, Grad-CAM seeks to identify
which parts of the image contributed most to the score of that class."),
tags$li("Choosing the Last Convolutional Layer: Grad-CAM focuses on the feature maps in the last convolutional layer of the CNN.
This layer retains spatial information that is essential for visualizing 'where' certain features were detected in the image."),
tags$li("Computing Gradients: Grad-CAM calculates the gradient of the class score with respect to each feature map in the last convolutional layer.
Mathematically, this is represented as the gradient, $$ \\frac{\\partial y_c}{\\partial A_k} $$, where $$ y_c $$ is the score of the class of interest
and $$ A_k $$ is the activation of the $$ k $$-th feature map in the selected layer. This gradient highlights the sensitivity of the prediction to changes in each feature map."),
tags$li("Global Average Pooling of Gradients: The gradients for each feature map are averaged across all spatial locations to get a single importance weight for each map.
This weight is computed as: $$ \\alpha_k^c = \\frac{1}{Z} \\sum_{i,j} \\frac{\\partial y_c}{\\partial A_{k}^{ij}} $$,
where $$ Z $$ is the total number of spatial locations, and $$ \\alpha_k^c $$ represents the importance of each feature map $$ A_k $$ for the class $$ c $$."),
tags$li("Generating the Heatmap: Grad-CAM produces a weighted combination of feature maps by multiplying each feature map $$ A_k $$ by its corresponding weight $$ \\alpha_k^c $$.
The heatmap $$ L^c $$ is then calculated as: $$ L^c = \\text{ReLU} \\left( \\sum_k \\alpha_k^c A_k \\right) $$,
where $$ \\text{ReLU} $$ sets negative values to zero, focusing on positive contributions to the prediction. This step effectively highlights areas in the image that most support the model’s classification decision."),
tags$li("Overlaying the Heatmap: The heatmap is resized to match the original image and is superimposed to highlight areas that most influenced the model's decision.
Brighter regions indicate the model's focus, helping users interpret the relevance of different parts of the image.")
),
h4("Why Grad-CAM is Important:"),
tags$ul(
tags$li("Model Interpretability: Grad-CAM provides insights into the decision-making process of neural networks, making the model’s reasoning more transparent."),
tags$li("Debugging Models: By showing where the model focuses, Grad-CAM can reveal if the model is relying on irrelevant or misleading parts of the image,
helping identify issues in the model's understanding."),
tags$li("Trust and Transparency: This technique builds trust in model predictions by explaining how the model arrived at its decision, which is especially useful in critical applications.")
),
h4("Further Reading"),
p("For a comprehensive explanation of Grad-CAM, you can refer to the original research paper:"),
tags$a(href = "https://arxiv.org/abs/1610.02391", "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization")
)
)
)
)
)
)
# Chunk 4
# Define server logic
# server <- function(input, output, session) {
#
#   observeEvent(input$submit, {
#     req(input$file)  # Ensure an image file is uploaded
#
#     # Save the uploaded file to www folder with a fixed name for consistency
#     original_image_path <- file.path("www", "original_resized.png")
#     file.copy(input$file$datapath, original_image_path, overwrite = TRUE)
#
#     # Show progress bar while the model is running
#     withProgress(message = 'Analyzing image...', value = 0, {
#
#       # Step 1: Load the selected model based on user input
#       incProgress(0.2, detail = "Loading model")
#
#       # Call load_model from Python with the selected model name
#       python_model <- py$load_model(input$model_choice)
#       model <- python_model[[1]]
#       last_conv_layer_name <- python_model[[2]]
#       preprocess <- python_model[[3]]
#
#
#       # Step 2: Run Grad-CAM analysis with the specified model and parameters
#       incProgress(0.3, detail = "Running Grad-CAM")
#       source_python("C:/Users/rodri/OneDrive - St. Lawrence University/STAT289/SYE/model_script.py")  # Path to your Python script
#
#       # Call the Python function with the selected confidence threshold
#       results <- generate_gradcam(input$file$datapath, model, top_n=5, confidence_threshold=input$confidence, last_conv_layer_name=last_conv_layer_name)
#
#       print(paste("Results length:", length(results)))  # Debugging check
#
#       # Step 3: Display notifications and render results
#       if (length(results) == 0) {
#         showNotification("No predictions found above the confidence threshold.", type = "error")
#       } else {
#         incProgress(0.6, detail = "Rendering outputs")
#
#         # Display the uploaded image
#         output$uploaded_image <- renderImage({
#           list(src = original_image_path, alt = "Uploaded Image", width = "400px", height = "400px")
#         }, deleteFile = FALSE)
#
#         # Dynamically render predictions and their Grad-CAM images in the Prediction Results tab
#         output$predicted_results <- renderUI({
#           lapply(1:length(results), function(i) {
#             result <- results[[i]]
#
#             # Prepare Grad-CAM image rendering using renderImage
#             output[[paste0("gradcam_output_", i)]] <- renderImage({
#               if (file.exists(result$gradcam_img_path)) {
#                 list(src = result$gradcam_img_path, height = "300px", width = "300px", alt = "Grad-CAM Output")
#               } else {
#                 NULL  # Return NULL if the file doesn't exist
#               }
#             }, deleteFile = FALSE)
#
#             # Dynamically create UI elements for each prediction
#             tagList(
#               fluidRow(
#                 column(6,
#                        h4(paste("Predicted Category:", result$label)),
#                        h5(paste("Confidence Score:", round(result$confidence * 100, 2), "%"))
#                 ),
#                 column(6,
#                        h4("Grad-CAM Output:"),
#                        imageOutput(paste0("gradcam_output_", i), height = "300px", width = "300px")
#                 )
#               ),
#               hr()  # Add a horizontal line between predictions
#             )
#           })
#         })
#
#         incProgress(1, detail = "Complete!")
#       }
#     })
#   })
# }
# Chunk 5
server <- function(input, output, session) {
observeEvent(input$submit, {
req(input$file)
# Save the uploaded file to www folder with a fixed name for consistency
original_image_path <- file.path("www", "original_resized.png")
file.copy(input$file$datapath, original_image_path, overwrite = TRUE)
# Load the Python script with load_model and generate_gradcam functions
py_run_file("C:/Users/rodri/OneDrive - St. Lawrence University/STAT289/SYE/model_script.py")
if (is.null(py$load_model) || is.null(py$generate_gradcam)) {
showNotification("Error loading Python functions. Check script path or function definitions.", type = "error")
return()
}
# Show progress bar while the model is running
withProgress(message = 'Analyzing image...', value = 0, {
incProgress(0.2, detail = "Loading model")
# Load the model, layer, preprocessing function, and input size
python_model <- py$load_model(input$model_choice)
model <- python_model[[1]]
last_conv_layer_name <- python_model[[2]]
preprocess <- python_model[[3]]
input_size <- python_model[[4]]
incProgress(0.3, detail = "Running Grad-CAM")
# Call generate_gradcam with model and preprocess arguments
results <- py$generate_gradcam(input$file$datapath, model, preprocess, top_n=5, confidence_threshold=input$confidence, last_conv_layer_name=last_conv_layer_name, input_size=input_size)
if (length(results) == 0) {
showNotification("No predictions found above the confidence threshold.", type = "error")
} else {
incProgress(0.6, detail = "Rendering outputs")
# Update the original image every time a new file is uploaded
output$uploaded_image <- renderImage({
list(src = original_image_path, alt = "Uploaded Image", width = "400px", height = "400px")
}, deleteFile = FALSE)
# Dynamically render predictions and their Grad-CAM images in the Prediction Results tab
output$predicted_results <- renderUI({
lapply(1:length(results), function(i) {
result <- results[[i]]
# Prepare Grad-CAM image rendering using renderImage
output[[paste0("gradcam_output_", i)]] <- renderImage({
# Ensure the image file exists before serving it
if (file.exists(result$gradcam_img_path)) {
list(src = result$gradcam_img_path, height = "300px", width = "300px", alt = "Grad-CAM Output")
} else {
NULL  # Return NULL if the file doesn't exist
}
}, deleteFile = FALSE)
# Dynamically create UI elements for each prediction
tagList(
fluidRow(
column(6,
h4(paste("Predicted Category:", result$label)),
h5(paste("Confidence Score:", round(result$confidence * 100, 2), "%"))
),
column(6,
h4("Grad-CAM Output:"),
# Output the image using imageOutput instead of img()
imageOutput(paste0("gradcam_output_", i), height = "300px", width = "300px")
)
),
hr()  # Add a horizontal line between predictions
)
})
})
incProgress(1, detail = "Complete!")
}
})
})
}
# Chunk 6
# Run the Shiny app
shinyApp(ui = ui, server = server)
shinyApp(ui = ui, server = server)
q
X
q
q()
library(reticulate)
py_config()
library(reticulate)
use_virtualenv("/srv/shiny-server/SYE/.venv", required = TRUE)
py_run_string("import shap; print('SHAP is accessible inside R')")
q()
library(reticulate)
use_virtualenv("/srv/shiny-server/SYE/.venv", required = TRUE)
# Print out sys.path for debugging
py_run_string("
import sys
print('sys.path in reticulate:')
for p in sys.path:
    print('-', p)
")
# Attempt to import shap
py_run_string("
import shap
print('SHAP imported successfully in reticulate!')
")
library(reticulate)
use_virtualenv("/srv/shiny-server/SYE/.venv", required = TRUE)
# Check if R is using the correct Python
py_config()
# Run a test import
py_run_string("import shap; print('SHAP is accessible inside R')")
q()
library(reticulate)
py_install("shap", envname = "/srv/shiny-server/SYE/.venv")
py_run_string("import shap; print('SHAP is accessible inside R')")
q()
library(reticulate)
use_virtualenv("/srv/shiny-server/SYE/.venv", required = TRUE)
py_run_string("import matplotlib; print('Matplotlib is accessible inside R')")
library(reticulate)
use_python("/srv/shiny-server/SYE/.venv/bin/python", required = TRUE)
py_run_string("import matplotlib; print('Matplotlib is accessible')")
library(reticulate)
use_python("/srv/shiny-server/SYE/.venv/bin/python", required = TRUE)
py_run_string("import matplotlib; print('Matplotlib is accessible')")
q()
library(reticulate)
use_python("/srv/shiny-server/SYE/.venv/bin/python", required = TRUE)
py_run_string("import matplotlib; print('Matplotlib is accessible')")
q()
library(reticulate)
use_virtualenv("/srv/shiny-server/SYE/.venv", required = TRUE)
py_config()  # Check if it points to the correct environment
py_run_string("import matplotlib; print('Matplotlib is accessible')")

library(reticulate)
virtualenv_install("/srv/shiny-server/SYE/.venv", packages = "matplotlib")

py_run_string("import matplotlib; print('Matplotlib is accessible')")

q()
